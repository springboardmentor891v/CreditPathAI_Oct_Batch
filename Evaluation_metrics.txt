EVALUATION METRICS: (Classification metrics)

1. Accuracy: The ratio of correctly predicted observations to the total number of observations. It is most useful for balanced datasets. 
2. Precision:  Out of all the instances the model predicted as positive, what proportion were actually positive. It is useful when the cost of a false positive is high
3. Recall / (Sensitivity): Out of all the actual positive instances, what proportion did the model correctly predict. It is useful when the cost of a false negative is high. 
4. F1-score: The harmonic mean of precision and recall, providing a single score that balances both metrics. It's particularly useful for imbalanced datasets. 

1. True Positive (TP): The model correctly predicted a positive outcome.
2. True Negative (TN): The model correctly predicted a negative outcome.
3. False Positive (FP): The model incorrectly predicted a positive outcome (Type I error).
4. False Negative (FN): The model incorrectly predicted a negative outcome (Type II error). 

TPR (True Positive Rate) is the proportion of actual positives correctly identified by a model, while 
FPR (False Positive Rate) is the proportion of actual negatives that are incorrectly identified as positive. TPR measures a model's ability to detect positive cases (sensitivity or recall), whereas FPR measures the rate of false alarms. 

ROC Curve: 
A ROC (Receiver Operating Characteristic) curve is a graph that visualizes the performance of a binary classification model by plotting the true positive rate (TPR) against the false positive rate (FPR) at various threshold settings. It is a powerful tool for evaluating model quality, comparing different classifiers, and selecting an optimal threshold for a specific use case. The curve helps determine the trade-off between sensitivity (TPR) and specificity (1-FPR). 

ROC-AUC Curve:
The Receiver Operating Characteristic (ROC) curve plots the true positive rate against the false positive rate at various threshold settings. The Area Under the Curve (AUC) is a common metric derived from this. 

Correlation Matrix:
A correlation matrix is a table showing the correlation coefficients between variables, which measure the linear relationship's direction and strength. The values range from -1 to +1, where -1 is a perfect negative correlation, +1 is a perfect positive correlation, and 0 is no correlation. It is used to summarize a dataset, visualize patterns, and as a diagnostic tool in statistical analysis. 