
# ============================================================================
# CREDIT PATH AI - GERMAN CREDIT DATASET WITH ALL VISUALIZATIONS
# ============================================================================

!pip install -q pandas numpy scikit-learn xgboost matplotlib seaborn

import pandas as pd
import numpy as np
import pickle
import warnings
warnings.filterwarnings('ignore')

from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
import xgboost as xgb
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score,
    confusion_matrix, roc_curve, auc, classification_report
)
import matplotlib.pyplot as plt
import seaborn as sns

print("âœ“ All imports successful!\n")

# ============================================================================
# STEP 1: UPLOAD AND LOAD GERMAN CREDIT DATA
# ============================================================================

print("="*80)
print("STEP 1: LOADING GERMAN CREDIT DATASET")
print("="*80 + "\n")

from google.colab import files

print("Upload german.data file (1.2 MB)...")
uploaded = files.upload()

# Load German Credit data
df = pd.read_csv('german.data', sep=' ', header=None)
print(f"âœ“ Data loaded: {df.shape}\n")

# ============================================================================
# STEP 2: NAME COLUMNS & PREPARE DATA
# ============================================================================

print("="*80)
print("STEP 2: PREPARING GERMAN CREDIT DATA")
print("="*80 + "\n")

# Name columns
column_names = [
    'existing_checking', 'duration', 'credit_history', 'purpose',
    'credit_amount', 'savings', 'employment_duration', 'installment_rate',
    'personal_status', 'debtors', 'residence_duration', 'property',
    'age', 'other_plans', 'housing', 'existing_credits', 'job',
    'dependents', 'telephone', 'foreign_worker', 'credit_status'
]

df.columns = column_names

print(f"Columns: {df.columns.tolist()}")
print(f"Shape: {df.shape}\n")

# Select and prepare data
df_prepared = pd.DataFrame({
    'age': df['age'],
    'credit_amount': df['credit_amount'],
    'duration': df['duration'],
    'employment_duration': df['employment_duration'],
    'installment_rate': df['installment_rate'],
    'loan_status': (df['credit_status'] == 2).astype(int)  # 2 = Bad credit (default)
})

print(f"Data prepared:")
print(f"  Shape: {df_prepared.shape}")
print(f"  Default rate: {df_prepared['loan_status'].mean():.2%}")
print(f"  Non-default: {(df_prepared['loan_status'] == 0).sum()}")
print(f"  Default: {(df_prepared['loan_status'] == 1).sum()}\n")

# ============================================================================
# STEP 3: ADD ESTIMATED FEATURES
# ============================================================================

print("="*80)
print("STEP 3: FEATURE ENGINEERING")
print("="*80 + "\n")

# Estimate income from credit amount and installment rate
df_prepared['income'] = df_prepared['credit_amount'] / (df_prepared['installment_rate'] + 0.1)

# Estimate credit score
np.random.seed(42)
df_prepared['credit_score'] = 600 + np.random.randint(-100, 100, len(df_prepared))

# Add delinquencies (estimated)
df_prepared['num_delinquencies'] = np.random.randint(0, 3, len(df_prepared))

# Loan amount = credit amount
df_prepared['loan_amount'] = df_prepared['credit_amount']

# Create debt to income ratio
df_prepared['debt_to_income_ratio'] = df_prepared['loan_amount'] / (df_prepared['income'] + 1)

# Rename for final dataset
df_final = df_prepared[[
    'age', 'income', 'loan_amount', 'credit_score',
    'employment_duration', 'num_delinquencies', 'debt_to_income_ratio', 'loan_status'
]].copy()

df_final.columns = [
    'age', 'income', 'loan_amount', 'credit_score',
    'employment_years', 'num_delinquencies', 'debt_to_income_ratio', 'loan_status'
]

print(f"Final dataset:")
print(f"  Shape: {df_final.shape}")
print(f"  Columns: {df_final.columns.tolist()}\n")
print(f"First few rows:")
print(df_final.head())

# ============================================================================
# STEP 4: DATA PREPROCESSING
# ============================================================================

print("\n" + "="*80)
print("STEP 4: DATA PREPROCESSING")
print("="*80 + "\n")

df = df_final.copy()

# Encode 'employment_years' before outlier removal
le = LabelEncoder()
df['employment_years'] = le.fit_transform(df['employment_years'])
print("âœ“ 'employment_years' column encoded.")

# Remove outliers
numeric_cols = ['age', 'income', 'loan_amount', 'credit_score', 'employment_years']

for col in numeric_cols:
    Q1 = df[col].quantile(0.25)
    Q3 = df[col].quantile(0.75)
    IQR = Q3 - Q1
    lower = Q1 - 1.5 * IQR
    upper = Q3 + 1.5 * IQR
    df = df[(df[col] >= lower) & (df[col] <= upper)]

print(f"âœ“ Outliers removed: {df.shape}")

# Normalize
scaler = StandardScaler()
df[numeric_cols] = scaler.fit_transform(df[numeric_cols])

print("âœ“ Features normalized")
print(f"Final shape: {df.shape}\n")

# ============================================================================
# STEP 5: PREPARE TRAINING DATA
# ============================================================================

print("="*80)
print("STEP 5: PREPARING TRAINING DATA")
print("="*80 + "\n")

X = df.drop(columns=['loan_status'], errors='ignore')
y = df['loan_status']

feature_names = X.columns.tolist()

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

print(f"Training set: {X_train.shape}")
print(f"Test set: {X_test.shape}")
print(f"Default rate in training: {y_train.mean():.2%}")
print(f"Default rate in test: {y_test.mean():.2%}\n")

# ============================================================================
# STEP 6: TRAIN ALL 5 MODELS
# ============================================================================

print("="*80)
print("STEP 6: TRAINING ALL 5 MODELS")
print("="*80 + "\n")

models = {}
y_pred_all = {}
y_pred_proba_all = {}

# 1. XGBoost
print("1ï¸âƒ£ XGBoost...")
models['XGBoost'] = xgb.XGBClassifier(
    n_estimators=100, max_depth=5, learning_rate=0.1,
    subsample=0.8, colsample_bytree=0.8, random_state=42, n_jobs=-1
)
models['XGBoost'].fit(X_train, y_train)
y_pred_all['XGBoost'] = models['XGBoost'].predict(X_test)
y_pred_proba_all['XGBoost'] = models['XGBoost'].predict_proba(X_test)[:, 1]
print("   âœ“ Done!")

# 2. Random Forest
print("2ï¸âƒ£ Random Forest...")
models['Random_Forest'] = RandomForestClassifier(
    n_estimators=100, max_depth=8, random_state=42, n_jobs=-1
)
models['Random_Forest'].fit(X_train, y_train)
y_pred_all['Random_Forest'] = models['Random_Forest'].predict(X_test)
y_pred_proba_all['Random_Forest'] = models['Random_Forest'].predict_proba(X_test)[:, 1]
print("   âœ“ Done!")

# 3. SVM
print("3ï¸âƒ£ Support Vector Machine (SVM)...")
models['SVM'] = SVC(kernel='rbf', C=100, probability=True, random_state=42)
models['SVM'].fit(X_train_scaled, y_train)
y_pred_all['SVM'] = models['SVM'].predict(X_test_scaled)
y_pred_proba_all['SVM'] = models['SVM'].predict_proba(X_test_scaled)[:, 1]
print("   âœ“ Done!")

# 4. Logistic Regression
print("4ï¸âƒ£ Logistic Regression...")
models['Logistic_Regression'] = LogisticRegression(
    C=0.1, max_iter=1000, solver='lbfgs', random_state=42
)
models['Logistic_Regression'].fit(X_train_scaled, y_train)
y_pred_all['Logistic_Regression'] = models['Logistic_Regression'].predict(X_test_scaled)
y_pred_proba_all['Logistic_Regression'] = models['Logistic_Regression'].predict_proba(X_test_scaled)[:, 1]
print("   âœ“ Done!")

# 5. Decision Tree
print("5ï¸âƒ£ Decision Tree...")
models['Decision_Tree'] = DecisionTreeClassifier(max_depth=5, random_state=42)
models['Decision_Tree'].fit(X_train, y_train)
y_pred_all['Decision_Tree'] = models['Decision_Tree'].predict(X_test)
y_pred_proba_all['Decision_Tree'] = models['Decision_Tree'].predict_proba(X_test)[:, 1]
print("   âœ“ Done!\n")

# ============================================================================
# VISUALIZATION 1: CONFUSION MATRICES
# ============================================================================

print("="*80)
print("VISUALIZATION 1: CONFUSION MATRICES")
print("="*80 + "\n")

fig, axes = plt.subplots(2, 3, figsize=(18, 10))
fig.suptitle('Confusion Matrices - All 5 Models (German Credit Data)',
             fontsize=16, fontweight='bold')

models_list = ['XGBoost', 'Random_Forest', 'SVM', 'Logistic_Regression', 'Decision_Tree']

for idx, model_name in enumerate(models_list):
    ax = axes[idx // 3, idx % 3]

    cm = confusion_matrix(y_test, y_pred_all[model_name])

    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax,
                xticklabels=['Non-Default (0)', 'Default (1)'],
                yticklabels=['Non-Default (0)', 'Default (1)'],
                cbar_kws={'label': 'Count'},
                annot_kws={'size': 14})

    ax.set_title(f'{model_name}', fontweight='bold', fontsize=12)
    ax.set_ylabel('True Label')
    ax.set_xlabel('Predicted Label')

    tn, fp, fn, tp = cm.ravel()
    acc = (tp + tn) / (tp + tn + fp + fn)
    ax.text(0.5, -0.3, f'Accuracy: {acc:.3f}',
            ha='center', fontsize=10, transform=ax.transAxes)

axes[1, 2].axis('off')

plt.tight_layout()
plt.savefig('01_confusion_matrices_german_credit.png', dpi=150, bbox_inches='tight')
plt.show()

print("âœ“ Saved: 01_confusion_matrices_german_credit.png\n")

# ============================================================================
# VISUALIZATION 2: ROC CURVES
# ============================================================================

print("="*80)
print("VISUALIZATION 2: ROC CURVES")
print("="*80 + "\n")

fig, ax = plt.subplots(figsize=(12, 8))

colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#FFA07A', '#98D8C8']

for idx, model_name in enumerate(models_list):
    fpr, tpr, _ = roc_curve(y_test, y_pred_proba_all[model_name])
    roc_auc = auc(fpr, tpr)

    ax.plot(fpr, tpr, color=colors[idx], lw=2.5,
            label=f'{model_name} (AUC = {roc_auc:.3f})')

ax.plot([0, 1], [0, 1], 'k--', lw=2, label='Random Classifier')

ax.set_xlim([-0.02, 1.02])
ax.set_ylim([-0.02, 1.02])
ax.set_xlabel('False Positive Rate', fontsize=12, fontweight='bold')
ax.set_ylabel('True Positive Rate', fontsize=12, fontweight='bold')
ax.set_title('ROC Curves - German Credit Dataset', fontsize=14, fontweight='bold')
ax.legend(loc='lower right', fontsize=11)
ax.grid(alpha=0.3)

plt.tight_layout()
plt.savefig('02_roc_curves_german_credit.png', dpi=150, bbox_inches='tight')
plt.show()

print("âœ“ Saved: 02_roc_curves_german_credit.png\n")

# ============================================================================
# VISUALIZATION 3: PERFORMANCE METRICS
# ============================================================================

print("="*80)
print("VISUALIZATION 3: PERFORMANCE METRICS")
print("="*80 + "\n")

performance = {}

for model_name in models_list:
    y_pred = y_pred_all[model_name]
    y_pred_proba = y_pred_proba_all[model_name]

    performance[model_name] = {
        'Accuracy': accuracy_score(y_test, y_pred),
        'Precision': precision_score(y_test, y_pred),
        'Recall': recall_score(y_test, y_pred),
        'F1-Score': f1_score(y_test, y_pred),
        'ROC-AUC': roc_auc_score(y_test, y_pred_proba)
    }

performance_df = pd.DataFrame(performance).T

print("Performance Metrics Table:\n")
print(performance_df.round(4))
print()

# Visualize
fig, axes = plt.subplots(2, 3, figsize=(18, 10))
fig.suptitle('Performance Metrics - German Credit Dataset', fontsize=16, fontweight='bold')

metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC']

for idx, metric in enumerate(metrics):
    ax = axes[idx // 3, idx % 3]

    data = performance_df[metric].sort_values(ascending=False)
    colors_bar = ['#4ECDC4' if i == 0 else '#FF6B6B' for i in range(len(data))]

    ax.barh(data.index, data.values, color=colors_bar)
    ax.set_title(f'{metric}', fontweight='bold')
    ax.set_xlim([0, 1.0])

    for i, v in enumerate(data.values):
        ax.text(v + 0.02, i, f'{v:.3f}', va='center')

    ax.grid(axis='x', alpha=0.3)

axes[1, 2].axis('off')

plt.tight_layout()
plt.savefig('03_performance_metrics_german_credit.png', dpi=150, bbox_inches='tight')
plt.show()

print("âœ“ Saved: 03_performance_metrics_german_credit.png\n")

# ============================================================================
# VISUALIZATION 4: METRICS HEATMAP
# ============================================================================

print("="*80)
print("VISUALIZATION 4: METRICS HEATMAP")
print("="*80 + "\n")

fig, ax = plt.subplots(figsize=(12, 6))

metrics_heatmap = performance_df.T

sns.heatmap(metrics_heatmap, annot=True, fmt='.4f', cmap='RdYlGn',
            center=0.7, vmin=0.4, vmax=1.0,
            cbar_kws={'label': 'Score'}, ax=ax,
            linewidths=1, linecolor='gray')

ax.set_title('Metrics Heatmap - German Credit Dataset', fontsize=14, fontweight='bold')
ax.set_ylabel('Metrics')
ax.set_xlabel('Models')

plt.tight_layout()
plt.savefig('04_metrics_heatmap_german_credit.png', dpi=150, bbox_inches='tight')
plt.show()

print("âœ“ Saved: 04_metrics_heatmap_german_credit.png\n")

# ============================================================================
# VISUALIZATION 5: FEATURE IMPORTANCE
# ============================================================================

print("="*80)
print("VISUALIZATION 5: FEATURE IMPORTANCE")
print("="*80 + "\n")

fig, axes = plt.subplots(1, 2, figsize=(14, 6))

# XGBoost
xgb_importance = models['XGBoost'].feature_importances_
top_indices_xgb = np.argsort(xgb_importance)[-8:]

ax = axes[0]
ax.barh(range(len(top_indices_xgb)), xgb_importance[top_indices_xgb], color='#FF6B6B')
ax.set_yticks(range(len(top_indices_xgb)))
ax.set_yticklabels(np.array(feature_names)[top_indices_xgb])
ax.set_xlabel('Importance', fontweight='bold')
ax.set_title('XGBoost - Feature Importance', fontweight='bold')
ax.grid(axis='x', alpha=0.3)

# Random Forest
rf_importance = models['Random_Forest'].feature_importances_
top_indices_rf = np.argsort(rf_importance)[-8:]

ax = axes[1]
ax.barh(range(len(top_indices_rf)), rf_importance[top_indices_rf], color='#4ECDC4')
ax.set_yticks(range(len(top_indices_rf)))
ax.set_yticklabels(np.array(feature_names)[top_indices_rf])
ax.set_xlabel('Importance', fontweight='bold')
ax.set_title('Random Forest - Feature Importance', fontweight='bold')
ax.grid(axis='x', alpha=0.3)

plt.tight_layout()
plt.savefig('05_feature_importance_german_credit.png', dpi=150, bbox_inches='tight')
plt.show()

print("âœ“ Saved: 05_feature_importance_german_credit.png\n")

# ============================================================================
# FINAL SUMMARY
# ============================================================================

print("="*80)
print("FINAL SUMMARY - GERMAN CREDIT DATASET")
print("="*80 + "\n")

print("All visualizations generated!\n")

print("Generated PNG files:")
print("  1. 01_confusion_matrices_german_credit.png")
print("  2. 02_roc_curves_german_credit.png")
print("  3. 03_performance_metrics_german_credit.png")
print("  4. 04_metrics_heatmap_german_credit.png")
print("  5. 05_feature_importance_german_credit.png\n")

print("Performance Summary:\n")
print(performance_df.round(4))
print()

best_model = performance_df['Accuracy'].idxmax()
best_accuracy = performance_df['Accuracy'].max()

print(f"ðŸ† Best Model: {best_model}")
print(f"   Accuracy: {best_accuracy:.4f}\n")

print("="*80)
print("âœ“ PROJECT COMPLETE!")
print("="*80)


